<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Generalisation : ">

    <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

    <title>Generalisation in Multi-Layered Function Approximators</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/chr-nas-hay/generalisation">View on GitHub</a>

          <h1 id="project_title">Generalisation in Multi-Layered Function Approximators</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/chr-nas-hay/generalisation/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/chr-nas-hay/generalisation/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>1. Introduction</h3>
        <p>
          In some respects, how deep neural networks remains mysterious. In this report, by studying a simplified class of models related to neural networks, I aim to elucidate the way multi-layered function approximators form generalisations about training data.
        </p>
        <p>
          A key finding is that, given that each layer in a neural network can be viewed as a function, generalisation can be improved by reducing the image of each layer while still fitting the training set. Furthermore, I argue that any technique which successfully reduces over-fitting in deep neural networks achieves this outcome.
        </p>
        <!--TODO: references. "final section" should be "section 1.2.3"-->
        <!--TODO: reference that paper by those Asian guys who tried out that "activation scheme"-->
        <!--TODO: comment on relation to sparsity-->
        <p>
          In the final section of this report, I use my findings to elaborate on existing explanations for how techniques like dropout (Hinton, 2014), pruning, sparsity, and bottlenecks are able to improve generalisation.
        </p>

        <h3>2. A Simplified Class of Function Approximators</h3>
        <p>
          By restricting my attention to a subset of function approximators, I hope to increase the interpretability of a trained model. One key factor which reduces the interpretability of a trained neural network is that the many-dimensional manifold which it learns is difficult to visualise. Therefore, in this report, discussion will focus on a model made up by the composition of arbitrary non-linear functions from \(\mathbb{R}^2\) to \(\mathbb{R}\). Such functions are easy to visualise. More specifically, I will model a function of 8 inputs such that
          $$
            f(x_0, \ldots , x_7) = f_7(f_5(f_1(x_0, x_1), f_2(x_2, x_3)), f_6(f_3(x_4, x_5), f_4(x_6, x_7))).
          $$
          Therefore, learning \(f\) is achieved by learning each \(f_1, \ldots, f_7\). The structure of this model can be more easily understood using the diagram in Figure 1.
        </p>
        <img class="figure" src="./images/figures/1.png"/>
        <center>Figure 1: A schematic of the simplified model.</center>
        <p>
          The inputs and outputs to each \(f_1, \ldots, f_7\) are constrained to be in the interval \([0, 1]\).
        </p>
        <p>
          Note that, each \(f_1, \ldots, f_7\) is similar to a neural network with a single hidden layer in that either can represent an arbitrary non-linear function.
        </p>
        <p>
          <!--TODO: Links and shit-->
          The exact specification of the model is presented in the Appendix. The implementation in Python can be found here.
        </p>

        <h3>3. A Simple Training Task</h3>
        <p>
          Contrary to most current empirical research in machine learning, I also choose a task simple enough that its solution can be thoroughly analysed: The odd parity problem. However, I have modified the problem to aid visualisation of its solution; Instead of having inputs in the set \(\{0, 1\}\), the inputs are allowed to be in the interval \([0, 1]\) and must be rounded to the nearest integer by the model before their odd parity is computed as usual.
        </p>

        <h3>4. Training and Analysis</h3>
        <h4>4.1 Training Data</h4>
        <p>
          A very large training set was used to reduce the chance that my analysis would rely on artefacts in the dataset to draw spurious conclusions. Even though the integer version of the 8-bit parity problem requires only \(2^8 = 256\) unique examples to be learned perfectly, I have used 100,000 for this modified version. This number of examples is practically guaranteed to saturate the model's capacity.
        </p>
        <h4>4.2 Initialisation</h4>
        <p>
          I Initialised each \(f_1(x, y), \ldots, f_6(x, y) \approx (x + y)/2\)&mdash;That is, the average of their arguments. I initialised \(f_7(x, y) = 0.5\). This is shown in Figure 2.
        </p>
        <img class="figure" src="./images/figures/img-000000.png"/>
        <center>Figure 2: Plots corresponding to the functions in Figure 1. For each plot, the top-left corner represents the point (0, 0), and the bottom-right point represents (1, 1). The vertical-running axis of each plot represents the input from the left function. For example, in the bottom-most plot, the vertical axis represents the input of the function immediately above it. In each plot, black represents a value of 0.0, whereas white represent 1.0.</center>

        <h4>4.3 Training</h4>
        <p>
          Training was accomplished using stochastic gradient descent. A custom-made regulariser and normalisation scheme, akin to batch normalisation <a href="https://arxiv.org/abs/1502.03167">(Ioffe and Szegedy, 2015)</a>, were also implemented. These were not gradient-based.
        </p>
        <p>
          A full description of the training method used is largely aside from the focus of this report and has been omitted from this section. Nonetheless, it can be found the Appendix.
        </p>

        <h4>4.4 Analysis of A Trained Model</h4>
        <h5>4.4.1 The Trained Model</h5>
        <p>
          Figure 3 shows the functions learned after training.
        </p>
        <img class="figure" src="./images/figures/img-000500.png"/>
        <center>Figure 3: The trained model.</center>
        <p>
          It can be seen that the top-most row of four functions (which correspond to \(f_1, \ldots, f_4\) in Figure 1) have come to approximate the function
          $$
            \begin{align}
              f_{1:4}(x_0, x_1) &amp;=
              \begin{cases}
                0           &amp; x_0 \lt  \frac{1}{2}, x_1 \lt  \frac{1}{2} \\
                \frac{1}{2} &amp; x_0 \lt  \frac{1}{2}, x_1 \geq \frac{1}{2} \\
                \frac{1}{2} &amp; x_0 \geq \frac{1}{2}, x_1 \lt  \frac{1}{2} \\
                1           &amp; x_0 \geq \frac{1}{2}, x_1 \geq \frac{1}{2}
              \end{cases} \\
              &amp;= \mathrm{avg}(\mathrm{round}(x_0), \mathrm{round}(x_1)) \\
              &amp;= \frac{1}{2} \sum^{2 - 1}_{i = 0} \mathrm{round}(x_i).
            \end{align}
          $$
          This is equivalent to counting the number of arguments which round to 1.0 then normalising that quantity to be in the range 1.0.
        </p>
        <p>
          Each function in the middle layer also computes a normalised count:
          $$
            \begin{align}
            f_{5:6}(f_{1:4}(x_0, x_1), f_{1:4}(x_2, x_3)) &amp;= \frac{1}{4} \sum^{4 - 1}_{i = 0} \mathrm{round}(x_i).
            \end{align}
          $$
          It is only in the final layer that the count is determined to be odd or even. Figure 4 illustrates that, even in the 16-variate version of the problem, every layer but the last will learn to (approximately) count the rounded inputs.
        </p>
        <img class="figure" src="./images/figures/img-16-parity.png"/>
        <center>Figure 4: A solution to the 16-variable odd parity problem.</center>
        <p>
          It is worth noting that this solution is an artefact of the chosen initialisation. If I had chosen differently, the learned functions may have been different or may not have been successfully learned at all. In fact, the initialisation scheme was selected selected deliberately to facilitate easy training and produce a model which is straightforward to analyse.
        </p>

        <h5>4.4.2 Generalisation in The Trained Model</h5>
        <p>
          The way the trained model computes the parity of its input is worth noting because it can lead to sub-optimal generalisation. This is an important observation because there is nothing to prevent this kind of shortcoming in other multi-layered models like neural networks.
        </p>
        <p>
          To form a more precise characterisation of this shortcoming, consider the case where the dataset lacks examples whose rounded sum is equal to 4. Because the model learns to compute a count before checking for parity, one could not expect it to make the correct prediction in this situation. Consider also that the learned solution in Figure 3 would still be correct if the black regions in the first layer became white, or vice versa. Intuitively, this is because
          $$
            \mathrm{parity}(0, 1, x_2, x_3, x_4, x_5, x_6, x_7) = \mathrm{parity}(1, 0, x_2, x_3, x_4, x_5, x_6, x_7).
          $$
          Shading the black and white regions to be the same is a simplification in the sense that it reduces the number of possible outputs that the top-most layer can pass on to the next. More formally, it reduces the image of the layer under the training set. This can improve generalisation because the subsequent layer will only need learn to handle a reduced number of cases.
        </p>
        <p>
          Now that it has been identified that the model can be simplified by reducing the image of the first layer under the training set, a (crude) method can be devised to do so as part of a training procedure. I take the model from Figure 3 and, for each \(f_1, \ldots, f_4\), re-initialise it to 0.5 and perform training end-to-end until convergence. The resulting model can be seen in Figure 5. Notice that, because training was carried out end-to-end, the functions downstream from the first layer became more parsimonious as well.
        </p>
        <img class="figure" src="./images/figures/img-002500.png"/>
        <center>Figure 5: The model from Figure 3 re-trained to simplify its top-most layer.</center>
        <p>
        This procedure can be repeated on subsequent layers. However, it is important to note that subsequent layers are not having their image under the training set reduced. Rather, they are having their image under their input reduced. An animation of the entire procedure is shown in Figure 6. On this toy problem, the technique typically finds the simplest solution in the sense that the image of each layer under the training set is as small as it can be. However, due to to the stochasticity of the training procedure, this is only <u>likely</u> to happen, not guaranteed.
        </p>
        <img class="figure" src="./images/figures/full.gif"/>
        <center>Figure 6: The entire training procedure including function-wise re-training.</center>
        <img class="figure" src="./images/figures/img-003500.png"/>
        <center>Figure 7: The model after function-wise re-training has been completed.</center>

        <h3>5. Function-wise Re-training on An Under-Complete Dataset</h3>
        <p>
          In the previous section I argued that generalisation can be improved by using a function-wise re-training technique to reduce the image of each layer under the training set. However, because I used many examples, the only claim that was empirically tested was that the image of each layer under the training set could be reduced, not that doing so can improve generalisation. In this section, I aim to empirically verify both claims.
        </p>
        <p>
          To do so, I randomly generate a 100,000-example dataset and remove examples whose rounded elements sum to 0, 1, 2, 6, 7, or 8. That is, I only retain examples summing to 3, 4, or 5. I then carry out the training and re-training procedure outlined in the previous section. Re-initialisations occurred after the \(500^{\mathrm{th}}, 1000^{\mathrm{th}}, 1500^{\mathrm{th}}, \ldots, 3000^{\mathrm{th}}\) batches. Figure 8 shows the classification error on randomly generated samples whose rounded elements can have any sum between 0 and 8, inclusive. The solution of a typical run is qualitatively the same as the one shown in Figure 7. Figure 9 shows the model during training.
        </p>
        <img class="figure" src="./images/figures/cls-err.png"/>
        <center>Figure 8: Generalisation error on the model shown in Figure 9. The model was trained with limited data and the function-wise re-training technique.</center>
        <img class="figure" src="./images/figures/less.gif"/>
        <center>Figure 9: Function-wise re-training on model which was given an under-complete dataset.</center>

        <h3>6. Discussion</h3>
        <p>
          The experiments recorded in this report strongly suggest that the principle way (possibly the only way) to reduce generalisation error in <u>any</u> multi-layered model is to reduce the image of each layer under the training set. If this hypothesis holds, any method which successfully combats over-fitting in deep neural nets is an image-reduction technique.
        </p>
        <p>
          Strong evidence for this hypothesis can be found in <a href="https://arxiv.org/abs/1603.05201">Shang et al. (2016)</a>, who &ldquo;[examined] existing CNN models and [observed] an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase)&rdquo;. The filter pairs tended to be linearly dependent, which Shang et al. suggested harmed performance. Upon preventing these superfluous filters from being learned, Shang et al. noted a significant improvement in generalisation. The work recorded in this report suggests that it is not the fact that extraneous filters must be learned which hampers generalisation. Rather, it is the fact that the later layers must learn how to use the extra filters.
        </p>
        <p>
          <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">Srivastava et al. (2014)</a> hypothesise that Dropout works in the following way:
        </p>
        <blockquote>
          Standard backpropagation learning builds up brittle co-adaptations that work for the training data but do not generalize to unseen data. Random dropout breaks up these co-adaptations by making the presence of any particular hidden unit unreliable.
        </blockquote>
        <p>
          However, the work presented in this report suggests that it is better to interpret dropout as a technique which slightly corrupts the surface that each layer represents. Under this interpretation, the more intricate a surface is, the less likely its corrupted form is to closely represent its original form. This makes it least challenging to learn simple functions, which are qualitatively more similar to the top-most layer in Figure 3 than the bottom-most layer.
        </p>

        <p>
          Sparsity is a property which, similarly to dropout, is intended to improve generalisation. However, perhaps counter-intuitively, more sparsity does not necessarily imply better generalisation (<a href="http://jmlr.csail.mit.edu/proceedings/papers/v15/glorot11a/glorot11a.pdf">Glorot et al., 2011</a>; <a href="https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf">Mass at al., 2013</a>). The work in this report offers an explanation, as a sparse layer does not necessarily have a minimal image.
        </p>

        <h3>7. Future Work</h3>
        <p>
          A natural line of work to pursue is to develop an image-reduction technique which can be applied to large, state-of-the art neural networks. To date, I have made some preliminary attempts, however these have largely proven to be unsuccessful. Neural networks similar enough to the model presented here are difficult to train, even though easy to apply the re-training technique to in principle. However, sufficiently dissimilar networks are easy to train yet difficult to apply the re-training technique to.
        </p>
        <p>
          It is possible that re-training is a poor strategy, and image-reduction should instead be encouraged by augmenting the network's objective function.
        </p>
        <h3>Appendix</h3>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Generalisation in Multi-Layered Function Approximators maintained by <a href="https://github.com/chr-nas-hay">chr-nas-hay</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
